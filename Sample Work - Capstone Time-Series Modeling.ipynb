{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTO REGRESSIVE TIME SERIES MODELING FOR DEMAND FORECASTING\n",
    "\n",
    "CHINMAY BAKE\n",
    "\n",
    "\n",
    "**Please note: most cells have not been ran in this Jupyter notebook due to the fact that the graphics could reveal certain aspects of the data, which might be prohibited under a confidentiality agreement. This includes missing values on the y-axis of all the plots and incomplete texts in the printed values. Hence, the below graphics are just screenshots from the actual notebook. This notebook does not represent my complete work, it is just an illustration of the researched methods and their implementation during the capstone project.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "from statsmodels.tsa.ar_model import AutoReg,ar_select_order\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTION TO AUTO REGRESSION\n",
    "\n",
    "We have a series of data points which are ordered with time, meaning that there is a time dimension associated with each value. Such data could be called as Time Series. One of the very well known ways of modeling Time series is Auto Regression.\n",
    "Lets take a look at the plot below. What if I determine that my sales in March are dependent on my sales in February? Or maybe what if my sale for today would be dependent on my yesterday’s sale. An autogregression model works in similar way. A model that depends on one period or one lag in the past is called an Auto Regressive model of order 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))*100\n",
    "\n",
    "def MASE(training_series, testing_series, prediction_series):\n",
    "\n",
    "    n = training_series.shape[0]\n",
    "    d = np.abs(  np.diff( training_series) ).sum()/(n-1)\n",
    "   \n",
    "    errors = np.abs(testing_series - prediction_series )\n",
    "    return errors.mean()/d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCALE INDEPENDENT METRIC - MASE\n",
    "\n",
    "MASE stands for Mean Absolutely Scaled Error. It is a scale free error term.  Its best interpretation would be that if its value is lesser than 1, then the model has a good fit, if greater then it doesn’t. The interpretation is always in reference to a Naive Forecasting model, with 1 being performance equivalent to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2.plot(figsize=(15,6))\n",
    "plt.title('Products')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](cp5.PNG \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul = seasonal_decompose(x2,model='additive',extrapolate_trend='freq')\n",
    "fig, ax = plt.subplots(3,1)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "ax[0].plot(mul.seasonal)\n",
    "ax[0].set_title('Seasonality')\n",
    "ax[1].plot(mul.trend)\n",
    "ax[1].set_title('Trend')\n",
    "ax[2].plot(mul.resid)\n",
    "ax[2].set_title('Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](cp6.PNG \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECOMPOSITION METHODS\n",
    "\n",
    "We can have two approaches which would best define any time series data point. In an additive time series, the components add together to make the time series. So, if we have an increasing trend, we will still see roughly the same size peaks and troughs throughout the time series. As against that, in  a multiplicative time series, the components multiply together to make the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_dict = {'seasonal': mul.seasonal, 'trend': mul.trend, 'residual': mul.resid}\n",
    "\n",
    "prediction_results = []\n",
    "\n",
    "for component in ['seasonal', 'trend', 'residual']:\n",
    "    historic = component_dict[component].iloc[:int(len(x2) * 0.7)].to_list()\n",
    "    test = component_dict[component].iloc[int(len(x2) * 0.7):]\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(len(test)):\n",
    "\n",
    "        model = AutoReg(historic,[1,5,6,7,8,14,16,21,24,29])\n",
    "        model_fit = model.fit()\n",
    "        pred = model_fit.predict(start = len(historic), end = len(historic), dynamic=False)\n",
    "        predictions.append(pred[0])\n",
    "        historic.append(test[i])\n",
    "\n",
    "    predictions = pd.Series(predictions, index=test.index, name=component)\n",
    "    prediction_results.append(predictions)\n",
    "    test_score = np.sqrt(mean_squared_error(test, predictions))\n",
    "    print(f'Test for {component} MSE: {test_score}')\n",
    "    # plot results\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(test.iloc[:], label='Observed '+component)\n",
    "    plt.plot(predictions.iloc[:], color='red', label='Predicted '+component)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](cp8.PNG \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECOMPOSITION & MODELING STRATEGY \n",
    "\n",
    "We break that data down into its individual components. Upon that, we Auto Regress each decomposed component and re-compose them together to generate a forecast. The method used for recomposition, could either be additive or multiplicative, depending on what suits best for that product time series.\n",
    "\n",
    "![alt text](cp2.PNG \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomposed_preds = pd.concat(prediction_results,axis=1).sum(axis=1)\n",
    "recomposed_preds.name = 'recomposed_preds'\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(x2.iloc[int(len(x2) * 0.7):], label='Observed')\n",
    "plt.plot(recomposed_preds, color='red', label='Predicted')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "rmse=np.sqrt(mean_squared_error(x2.iloc[int(len(x2) * 0.7):], recomposed_preds))\n",
    "mase=MASE(pd.Series(historic),x2.iloc[int(len(x2) * 0.7):],recomposed_preds)\n",
    "print(\"RMSE: \", rmse)\n",
    "print (\"MASE: \", mase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](cp10.PNG \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmselist = []\n",
    "maselist = []\n",
    "\n",
    "splits = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "print(splits)\n",
    "\n",
    "for train_index, test_index in splits.split(x2):\n",
    "    train = x2[train_index]\n",
    "    test = x2[test_index]\n",
    "    s=x2[:len(train)+len(test)]\n",
    "    print('Observations: %d' % (len(train) + len(test)))\n",
    "    print('Training Observations: %d' % (len(train)))\n",
    "    print('Testing Observations: %d' % (len(test)))\n",
    "        \n",
    "for train_index, test_index in splits.split(x2): \n",
    "    \n",
    "    train = x2[train_index]\n",
    "    testdata = x2[test_index]\n",
    "    print(\"Training Size: \",len(train))\n",
    "    print(\"Test Size: \",len(testdata))\n",
    "    print('Observations: %d' % (len(train) + len(testdata)))\n",
    "    \n",
    "    s=x2[:len(train)+len(testdata)]\n",
    "    mul = seasonal_decompose(s,model='additive',extrapolate_trend='freq')\n",
    "    prediction_results = []\n",
    "    \n",
    "    component_dict = component_dict = {'seasonal': mul.seasonal, 'trend': mul.trend, 'residual': mul.resid}\n",
    "    \n",
    "    for component in ['seasonal', 'trend', 'residual']:\n",
    "        historic = component_dict[component].iloc[:len(train)].to_list()\n",
    "        test = component_dict[component].iloc[len(testdata):]\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(len(test)):\n",
    "            model = AutoReg(historic,[1,5,6,7,8,14,16,21,24])\n",
    "            model_fit = model.fit()\n",
    "            pred = model_fit.predict(start=len(historic), end=len(historic), dynamic=False)\n",
    "            predictions.append(pred[0])\n",
    "            historic.append(test[i])\n",
    "            \n",
    "        predictions = pd.Series(predictions, index=test.index, name=component)\n",
    "        prediction_results.append(predictions)\n",
    "        \n",
    "    recomposed_preds = pd.concat(prediction_results,axis=1).sum(axis=1)\n",
    "    recomposed_preds.name = 'recomposed_preds'\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(s.iloc[len(testdata):], label='Observed')\n",
    "    plt.plot(recomposed_preds, color='red', label='Predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    rmse=np.sqrt(mean_squared_error(s.iloc[len(testdata):], recomposed_preds))\n",
    "    rmselist.append(rmse)\n",
    "    mase=MASE(pd.Series(historic),s.iloc[len(testdata):],recomposed_preds)\n",
    "    maselist.append(mase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](cp14.PNG \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE:  2.8353330339700404\n",
      "Average MASE:  0.6709016871432474\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "print(\"Average RMSE: \", mean(rmselist))\n",
    "\n",
    "print(\"Average MASE: \", mean(maselist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUTOMATIC IDENTIFICATION OF THE DECOMPOSITION METHOD \n",
    "\n",
    "In order to generalize the recomposition strategy over a wide range of time series patterns, we construct a function which would intelligently determine what would be the appropriate re-composition approach for the entered product, model it using autoregression and finally generate forecasts for us. The function is also capable of modeling seasonal patterns based of the frequency of their occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acf1(x):\n",
    "    return np.square(sum(acf(x)))\n",
    "\n",
    "def ssacf(add,mult):\n",
    "    return np.where(acf1(add)<acf1(mult),\"additive\",\"multiplicative\")\n",
    "\n",
    "mul = seasonal_decompose(x2 + 0.1,model='multiplicative',extrapolate_trend='freq')\n",
    "add = seasonal_decompose(x2,model='additive',extrapolate_trend='freq')\n",
    "model_type = ssacf(add.resid,mul.resid)\n",
    "\n",
    "if model_type=='multiplicative':\n",
    "    model = seasonal_decompose(x2+0.1,model=str(model_type),extrapolate_trend='freq')\n",
    "else: \n",
    "    model = seasonal_decompose(x2,model=str(model_type),extrapolate_trend='freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FORECASTING AUTOMATION FUNCTION\n",
    "\n",
    "For detailed code, please refer: https://github.com/chinmaybake/Capstone---Time-series-Modeling\n",
    "\n",
    "##### OVERVIEW\n",
    "\n",
    "POSSIBLE LIMITATIONS -\n",
    "\n",
    "The function in itself does not perform any data stationarity transformations or evaluations. This essentially owes to the fact that there is a substantial variation in stationarity behaviors of the products in the given data, and attaining stationarity through transformations for each product might require a varied set of transformation operations. Hence, this step has been descoped from the functionality.\n",
    "\n",
    "POSSIBLE BENEFITS -\n",
    "\n",
    "The function automatically identifies the appropriate decomposition methodology for a given product. This is estimated by analysing autocorrelations between the residuals of each decomposition method and returning the ones with the least sum of squares.\n",
    "\n",
    "The seasonal arguments in the input allow the user to enforce control over the seasonal magnitude of the given product. For Eg: if a product X showcases yearly seasonality in store Y then the user could call the function as follows -\n",
    "\n",
    "forecast_func(X,Y,True,360)\n",
    "\n",
    "where the True parameter stands for the fact that the product showcases seasonality and 360 is the frequency of seasonality. As the data has daily samples, on a yearly scale, yearly seasonal frequency would be equivalent to 360 days. Please note that 360 should be enetered as an Integer without any unit.\n",
    "\n",
    "Also, the last argument is an optional parameter, if Seasonality is set to FALSE, the function would defualt pass the value for seasonal frequency as 0 days\n",
    "\n",
    "**REFERENCES** \n",
    "\n",
    "* https://www.statsmodels.org/stable/examples/notebooks/generated/autoregressions.html\n",
    "* https://www.statsmodels.org/stable/generated/statsmodels.tsa.ar_model.AutoReg.html\n",
    "* https://otexts.com/fpp2/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
